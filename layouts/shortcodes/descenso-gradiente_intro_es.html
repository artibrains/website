<link rel="stylesheet" href="/css/main.css">

<div class="medical-context">
    <h2> Contexto M茅dico: Optimizaci贸n de Par谩metros Terap茅uticos</h2>
    <div class="medical-scenario">
        <div class="scenario-text">
            <p><strong>Imagina:</strong> Tu modelo de IA requiere ajusta de forma autom谩tica sus par谩metros. Para
                realizar ajustes precisos, es crucial encontrar los valores 贸ptimos que minimicen la funci贸n de
                p茅rdida del modelo. </p>
            </p>
            <p class="highlight">El descenso del gradiente ayuda a identificar los valores que minimizan la funci贸n de
                p茅rdida, alcanzando un m铆nimo. El valor m铆nimo que se alcanza depende de la tasa de aprendizaje y del
                punto de partida.
            </p>
        </div>
    </div>
</div>

<div id="app-explanation">
    <p class="intro">Explora en 1D y 2D c贸mo el descenso del gradiente converge hacia el m铆nimo de una funci贸n. Ajusta
        el tipo de tasa de aprendizaje y el punto de partida para comprender mejor su comportamiento en entornos m茅dicos
        reales.</p>

    <div class="how-it-works">
        <h2>驴C贸mo funciona?</h2>
        <div class="explanation-columns">
            <div class="column">
                <h3>Selecciona la funci贸n</h3>
                <p>Introduce una funci贸n que simule la p茅rdida del modelo (por ejemplo, error en niveles sangu铆neos) y
                    elige un punto inicial.</p>
            </div>
            <div class="column">
                <h3>Ajusta la tasa de aprendizaje</h3>
                <p>Prueba con tasa constante o decreciente (simple o exponencial) y observa c贸mo afecta la velocidad y
                    estabilidad de la convergencia.</p>
            </div>
            <div class="column">
                <h3>Observa el proceso</h3>
                <p>Visualiza paso a paso c贸mo el algoritmo desciende hasta el m铆nimo, controlando tambi茅n normatividad
                    del gradiente para detenerse en un punto 贸ptimo.</p>
            </div>
        </div>
    </div>

    <p>
        Despu茅s de que el equipo decidiera usar la regresi贸n lineal, Marta explic贸 c贸mo el <strong>m茅todo del descenso
            del gradiente</strong> les permitir铆a encontrar autom谩ticamente los mejores valores para los par谩metros
        <em>w</em> (peso) y <em>b</em> (sesgo). Como 茅l mismo dijo, es como "bajar una monta帽a a ciegas", donde en cada
        paso se busca la direcci贸n de m谩xima pendiente para descender.
    </p>
    <p>
        En esta simulaci贸n, puedes visualizar ese proceso. El gr谩fico de la izquierda muestra la superficie de la
        funci贸n de coste (en este caso, el Error Cuadr谩tico Medio o MSE). Cada punto en esta superficie representa el
        error total para una combinaci贸n espec铆fica de <em>w</em> y <em>b</em>. El objetivo es encontrar el punto m谩s
        bajo del valle.
    </p>
    <p>
        Usa los controles para iniciar el algoritmo. Observa c贸mo los par谩metros se actualizan en cada iteraci贸n
        (gr谩fico de la derecha), siguiendo la direcci贸n opuesta al gradiente, hasta converger en la soluci贸n 贸ptima que
        minimiza el error. Puedes experimentar con diferentes tasas de aprendizaje (<em>learning rate</em>) para ver
        c贸mo afecta a la velocidad y estabilidad del proceso, un punto que Marta y Luis discutieron para afinar su
        modelo.
    </p>
</div>